{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install -y gdown","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-25T09:48:30.826653Z","iopub.execute_input":"2024-04-25T09:48:30.827012Z","iopub.status.idle":"2024-04-25T09:51:32.177178Z","shell.execute_reply.started":"2024-04-25T09:48:30.826985Z","shell.execute_reply":"2024-04-25T09:51:32.176109Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Retrieving notices: ...working... done\nCollecting package metadata (current_repodata.json): / WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\ndone\nSolving environment: done\n\n\n==> WARNING: A newer version of conda exists. <==\n  current version: 23.7.4\n  latest version: 24.3.0\n\nPlease update conda by running\n\n    $ conda update -n base -c conda-forge conda\n\nOr to minimize the number of packages updated during conda update use\n\n     conda install conda=24.3.0\n\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    filelock-3.13.4            |     pyhd8ed1ab_0          15 KB  conda-forge\n    gdown-5.1.0                |     pyhd8ed1ab_0          21 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:          36 KB\n\nThe following NEW packages will be INSTALLED:\n\n  filelock           conda-forge/noarch::filelock-3.13.4-pyhd8ed1ab_0 \n  gdown              conda-forge/noarch::gdown-5.1.0-pyhd8ed1ab_0 \n\n\n\nDownloading and Extracting Packages\nfilelock-3.13.4      | 15 KB     |                                       |   0% \nfilelock-3.13.4      | 15 KB     | ##################################### | 100% \u001b[A\ngdown-5.1.0          | 21 KB     | ##################################### | 100% \u001b[A\n                                                                                \u001b[A\n                                                                                \u001b[A\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}]},{"cell_type":"code","source":" !gdown --id 1m95s8q2lGvNnG6m5JAzDidOdhlNL7Lxn","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:52:58.812105Z","iopub.execute_input":"2024-04-25T09:52:58.812467Z","iopub.status.idle":"2024-04-25T09:53:03.019554Z","shell.execute_reply.started":"2024-04-25T09:52:58.812439Z","shell.execute_reply":"2024-04-25T09:53:03.018614Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1m95s8q2lGvNnG6m5JAzDidOdhlNL7Lxn\nTo: /kaggle/working/train.csv\n100%|██████████████████████████████████████| 23.9M/23.9M [00:00<00:00, 67.5MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1a4Xf_v7ROi-unasxmb5-08-Hr4amcHan","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:53:25.555471Z","iopub.execute_input":"2024-04-25T09:53:25.555851Z","iopub.status.idle":"2024-04-25T09:53:29.859957Z","shell.execute_reply.started":"2024-04-25T09:53:25.555822Z","shell.execute_reply":"2024-04-25T09:53:29.859056Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1a4Xf_v7ROi-unasxmb5-08-Hr4amcHan\nTo: /kaggle/working/test.csv\n100%|███████████████████████████████████████| 1.50M/1.50M [00:00<00:00, 133MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1nCwQsQORexN2qyZqlYHNM_6QNm_T0zfC","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:15:35.900716Z","iopub.execute_input":"2024-04-25T10:15:35.901581Z","iopub.status.idle":"2024-04-25T10:15:47.649991Z","shell.execute_reply.started":"2024-04-25T10:15:35.901539Z","shell.execute_reply":"2024-04-25T10:15:47.648642Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1nCwQsQORexN2qyZqlYHNM_6QNm_T0zfC\nTo: /kaggle/working/preprocessed_train.csv\n100%|██████████████████████████████████████| 17.1M/17.1M [00:00<00:00, 49.1MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nimport string\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n# Load data\ndf = pd.read_csv(\"/kaggle/working/preprocessed_train.csv\")\ncorpus = df[\"Description\"].fillna('')\n\n# Create vocabulary and word2idx\nwords = set()\nfor sent in corpus:\n    sent = sent.translate(str.maketrans('', '', string.punctuation)).lower()\n    words.update([word for word in sent.split() if word.isalpha()])\n\n# Ensure '<unk>' and '<pad>' are in your dictionary\nword2idx = {word: i + 2 for i, word in enumerate(sorted(words))}  # Start indexing from 2\nword2idx['<unk>'] = 1  # Unknown words\nword2idx['<pad>'] = 0  # Padding token\nvocab_size=len(word2idx)\n# Convert sentences to sequences of indices\ndef sentence_to_idx(sent):\n    return [word2idx.get(word, word2idx['<unk>']) for word in sent if word in word2idx]\n\nprocessed_inputs = []\nprocessed_outputs = []\n\nfor sent in corpus:\n    sent = ['<sos>'] + sent.translate(str.maketrans('', '', string.punctuation)).lower().split() + ['<eos>']\n    idx_sent = sentence_to_idx(sent)\n    processed_inputs.append(torch.tensor(idx_sent[:-1], dtype=torch.long))\n    processed_outputs.append(torch.tensor(idx_sent[1:], dtype=torch.long))\n\n# Define the dataset\nclass TextDataset(Dataset):\n    def __init__(self, inputs, outputs):\n        self.inputs = inputs\n        self.outputs = outputs\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.outputs[idx]\n\n# Collate function to pad sequences\ndef collate_fn(batch):\n    inputs, outputs = zip(*batch)\n    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=word2idx['<pad>'])\n    outputs_padded = pad_sequence(outputs, batch_first=True, padding_value=word2idx['<pad>'])\n    return inputs_padded, outputs_padded\n\ntrain_dataset = TextDataset(processed_inputs, processed_outputs)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:35.572040Z","iopub.execute_input":"2024-04-25T10:16:35.572771Z","iopub.status.idle":"2024-04-25T10:16:41.973464Z","shell.execute_reply.started":"2024-04-25T10:16:35.572736Z","shell.execute_reply":"2024-04-25T10:16:41.972671Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:45.527611Z","iopub.execute_input":"2024-04-25T10:16:45.528760Z","iopub.status.idle":"2024-04-25T10:16:45.535680Z","shell.execute_reply.started":"2024-04-25T10:16:45.528727Z","shell.execute_reply":"2024-04-25T10:16:45.534669Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"52372"},"metadata":{}}]},{"cell_type":"code","source":"class ForwardLM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_prob):\n        super(ForwardLM, self).__init__()\n        self.embed_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.layer1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.layer2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n    \n    def forward(self, x):\n        embed = self.embed_layer(x)\n        lstm1, _ = self.layer1(embed)\n        lstm1 = self.dropout(lstm1)\n        lstm2, _ = self.layer2(lstm1)\n        lstm2 = self.dropout(lstm2)\n        output = self.fc(lstm2)\n        return output  # Returning only output for simplicity in loss calculation\n\n# Initialize the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ForwardLM(vocab_size, 300, 300, 0.5).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:49.310636Z","iopub.execute_input":"2024-04-25T10:16:49.311271Z","iopub.status.idle":"2024-04-25T10:16:49.972279Z","shell.execute_reply.started":"2024-04-25T10:16:49.311219Z","shell.execute_reply":"2024-04-25T10:16:49.971494Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])  # Ignore padding in loss computation\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\nmodel.train()\nfor epoch in range(5):\n    running_loss = 0.0\n    for inputs, outputs in train_loader:\n        inputs, outputs = inputs.to(device), outputs.to(device)\n        optimizer.zero_grad()\n        output = model(inputs)\n        loss = criterion(output.view(-1, vocab_size), outputs.view(-1))  # Flatten output for CrossEntropyLoss\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f'Epoch {epoch+1}: Loss {running_loss / len(train_loader.dataset):.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:16:56.573510Z","iopub.execute_input":"2024-04-25T10:16:56.574110Z","iopub.status.idle":"2024-04-25T10:27:28.479914Z","shell.execute_reply.started":"2024-04-25T10:16:56.574079Z","shell.execute_reply":"2024-04-25T10:27:28.478995Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1: Loss 0.2659\nEpoch 2: Loss 0.2525\nEpoch 3: Loss 0.2423\nEpoch 4: Loss 0.2350\nEpoch 5: Loss 0.2293\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model weights to a file\nmodel_path = '/kaggle/working/forward1.pt'\ntorch.save(model.state_dict(), model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:28:37.645277Z","iopub.execute_input":"2024-04-25T10:28:37.646151Z","iopub.status.idle":"2024-04-25T10:28:37.904386Z","shell.execute_reply.started":"2024-04-25T10:28:37.646114Z","shell.execute_reply":"2024-04-25T10:28:37.903570Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"BackwardLM\n****","metadata":{}},{"cell_type":"code","source":"# Assuming you already have processed_inputs and processed_outputs from your previous steps\nrev_inputs = [torch.flip(out, [0]) for out in processed_inputs]  # Reverse each input sequence\nrev_outputs = [torch.flip(inp, [0]) for inp in processed_outputs]  # Reverse each output sequence\n\nbackward_dataset = TextDataset(rev_inputs, rev_outputs)\nbackward_train_loader = DataLoader(backward_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:30:22.222941Z","iopub.execute_input":"2024-04-25T10:30:22.223337Z","iopub.status.idle":"2024-04-25T10:30:23.684797Z","shell.execute_reply.started":"2024-04-25T10:30:22.223307Z","shell.execute_reply":"2024-04-25T10:30:23.684040Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class BackwardLM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_prob):\n        super(BackwardLM, self).__init__()\n        self.embed_layer = nn.Embedding(vocab_size, embedding_dim)\n        self.layer1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.layer2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n    \n    def forward(self, x):\n        x = self.embed_layer(x)\n        x, _ = self.layer1(x)\n        x = self.dropout(x)\n        x, _ = self.layer2(x)\n        x = self.dropout(x)\n        return self.fc(x)  # Directly return the output for CrossEntropyLoss\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:30:26.126498Z","iopub.execute_input":"2024-04-25T10:30:26.126883Z","iopub.status.idle":"2024-04-25T10:30:26.135764Z","shell.execute_reply.started":"2024-04-25T10:30:26.126856Z","shell.execute_reply":"2024-04-25T10:30:26.134810Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbackward_model = BackwardLM(vocab_size, 300, 300, 0.5).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])\noptimizer = torch.optim.Adam(backward_model.parameters(), lr=0.0001)\n\nbackward_model.train()\nfor epoch in range(5):\n    running_loss = 0.0\n    for inputs, outputs in backward_train_loader:\n        inputs, outputs = inputs.to(device), outputs.to(device)\n        optimizer.zero_grad()\n        output = backward_model(inputs)\n        loss = criterion(output.view(-1, vocab_size), outputs.view(-1))\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    print(f'Epoch {epoch+1}: Loss {running_loss / len(backward_train_loader.dataset):.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:30:36.688048Z","iopub.execute_input":"2024-04-25T10:30:36.688424Z","iopub.status.idle":"2024-04-25T10:41:06.524463Z","shell.execute_reply.started":"2024-04-25T10:30:36.688397Z","shell.execute_reply":"2024-04-25T10:41:06.523545Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1: Loss 0.2447\nEpoch 2: Loss 0.1739\nEpoch 3: Loss 0.1108\nEpoch 4: Loss 0.0747\nEpoch 5: Loss 0.0553\n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = '/kaggle/working/backward1.pt'\ntorch.save(backward_model.state_dict(), model_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:42:05.184671Z","iopub.execute_input":"2024-04-25T10:42:05.185308Z","iopub.status.idle":"2024-04-25T10:42:05.439045Z","shell.execute_reply.started":"2024-04-25T10:42:05.185279Z","shell.execute_reply":"2024-04-25T10:42:05.438074Z"},"trusted":true},"execution_count":14,"outputs":[]}]}